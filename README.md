# ğŸ§  RAG Multiagente - hackÃœ

Este repositorio contiene la API REST de un sistema RAG (Retrieval-Augmented Generation) con agentes personalizados por cliente. Cada agente responde preguntas basÃ¡ndose exclusivamente en sus propios documentos. EstÃ¡ diseÃ±ado para integrarse fÃ¡cilmente con frontends como WhatsApp, Slack, Web o App.

---

## ğŸš€ CÃ³mo levantar el proyecto

### 1. Clona el repositorio

```bash
git clone https://github.com/hacku-team/hacku-rag.git
cd hacku-rag
2. Crea un entorno virtual (opcional pero recomendado)
python3 -m venv venv
source venv/bin/activate  # En Linux/Mac
venv\Scripts\activate     # En Windows

3. Instala las dependencias
pip install -r requirements.txt

4. Configura tus claves de API
Crea un archivo .env en la raÃ­z con el siguiente contenido:

OPENAI_API_KEY=tu-clave-openai
âš ï¸ Este archivo no debe ser versionado (ya estÃ¡ en el .gitignore).

5. Corre el servidor local
uvicorn api.main:app --reload --port 8001
Abre tu navegador en: http://localhost:8001/docs

ğŸ“¬ Endpoints principales
POST /query
Consulta estÃ¡ndar con recuperaciÃ³n de contexto por cliente.
{
  "cliente": "recetron",
  "pregunta": "Â¿QuÃ© es la inteligencia artificial moderna?"
}
Respuesta esperada:
{
  "cliente": "recetron",
  "respuesta": "La inteligencia artificial moderna se basa en modelos de aprendizaje profundo...",
  "fuentes": ["docs/archivo1.pdf", "docs/scrap.txt"]
}

ğŸ“ Estructura del repositorio

hacku_rag/
â”œâ”€â”€ api/                        # API REST con FastAPI
â”‚   â””â”€â”€ main.py                # Endpoints: /query, /query-stream
â”‚
â”œâ”€â”€ core/                       # LÃ³gica del sistema
â”‚   â”œâ”€â”€ ingest.py              # Ingesta + limpieza de documentos
â”‚   â”œâ”€â”€ embed_store.py         # IndexaciÃ³n en ChromaDB
â”‚   â”œâ”€â”€ query_engine.py        # Prompt + RetrievalQA + LLM
â”‚   â”œâ”€â”€ normalize_scraped_txt.py  # Limpieza de textos scrappeados
â”‚   â””â”€â”€ scraper.py             # Scraper bÃ¡sico de URLs
â”‚
â”œâ”€â”€ tasks/                      # Tareas asincrÃ³nicas (Celery)
â”‚   â””â”€â”€ scraper_task.py
â”‚
â”œâ”€â”€ scripts/                    # Utilitarios de prueba
â”‚   â””â”€â”€ run_scraper.py
â”‚
â”œâ”€â”€ clientes/                   # Datos por cliente
â”‚   â”œâ”€â”€ recetron/
â”‚   â”‚   â”œâ”€â”€ docs/              # Archivos .txt, .pdf, .docx
â”‚   â”‚   â”œâ”€â”€ chroma_db/         # Vector store local (Chroma)
â”‚   â”‚   â””â”€â”€ system_instruction.txt # Instrucciones del agente
â”‚   â”œâ”€â”€ bancolombia/
â”‚   â””â”€â”€ terpel/
â”‚
â”œâ”€â”€ .env                        # Claves de entorno (NO versionar)
â”œâ”€â”€ requirements.txt            # Dependencias Python
â””â”€â”€ README.md                   # Este archivo ğŸ“„

ğŸ§  Â¿CÃ³mo funciona?
Se detecta dinÃ¡micamente el cliente desde el payload.
Se cargan sus documentos vectorizados desde /clientes/[cliente]/docs.
Se aplica una instrucciÃ³n system_instruction.txt exclusiva por cliente.
Se genera la respuesta usando gpt-4o-mini de OpenAI vÃ­a LangChain.
Se devuelven los resultados junto a las fuentes utilizadas.

El sistema usa ChromaDB como vector store local, con limpieza automÃ¡tica de PDFs y segmentaciÃ³n optimizada vÃ­a SentenceTransformersTokenTextSplitter.

ğŸ›  TecnologÃ­as utilizadas
ğŸ§  LangChain + langchain-community
ğŸ§¬ OpenAI (modelo: gpt-4o-mini)
ğŸ§² ChromaDB
âš¡ FastAPI
ğŸ§ª Uvicorn
ğŸ“„ PDF / DOCX / TXT
ğŸ§¹ Limpieza con SentenceTransformers splitter

âœ… Pruebas
Puedes probar el sistema de tres formas:
http://localhost:8001/docs vÃ­a Swagger
Postman o Insomnia (POST /query)
Desde cualquier interfaz frontend (WhatsApp, web, etc.)

ğŸ” Seguridad
Cada agente tiene su propio vector store aislado.
Solo accede a sus documentos.
Se permite personalizar comportamiento con instrucciones por cliente.